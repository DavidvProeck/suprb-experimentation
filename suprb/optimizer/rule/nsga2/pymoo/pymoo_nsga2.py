import numpy as np
from typing import Optional

from suprb.rule import Rule, RuleInit
from suprb.rule.initialization import MeanInit
from suprb.optimizer.rule.nsga2.pymoo.rule_optimization_problem import RuleOptimizationProblem
from suprb.optimizer.rule.nsga2.pymoo.pymoo_mutation import PymooRuleMutation
from suprb.utils import RandomState
from src.suprb.suprb.optimizer.rule.base import ParallelSingleRuleDiscovery
from src.suprb.suprb.optimizer.rule.origin import SquaredError, RuleOriginGeneration
from src.suprb.suprb.optimizer.rule.mutation import RuleMutation, HalfnormIncrease
from src.suprb.suprb.optimizer.rule.constraint import CombinedConstraint, MinRange, Clip
from src.suprb.suprb.optimizer.rule.acceptance import Variance
from src.suprb.suprb.optimizer.rule import RuleAcceptance, RuleConstraint


from pymoo.algorithms.moo.nsga2 import NSGA2
from pymoo.optimize import minimize
from pymoo.termination import get_termination

import cProfile
import pstats



class PymooNSGA2(ParallelSingleRuleDiscovery):
    """
    Adapted from: A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II by Kalyanmoy Deb et al.
    This implementation is slower because repeated vectorization of the rule objects.
    Deprecated.
    """
    def __init__(
            self,
            n_iter: int = 100,
            mu: int = 20,
            lmbda: int = 40,
            origin_generation: RuleOriginGeneration = SquaredError(),
            init: RuleInit = MeanInit(),
            mutation: RuleMutation = HalfnormIncrease(sigma=1.22),
            constraint: RuleConstraint = CombinedConstraint(MinRange(), Clip()),
            acceptance: RuleAcceptance = Variance(),
            random_state: int = None,
            n_jobs: int = 1,
    ):
        super().__init__(
            n_iter=n_iter,
            origin_generation=origin_generation,
            init=init,
            acceptance=acceptance,
            constraint=constraint,
            random_state=random_state,
            n_jobs=n_jobs,
        )
        self.mu = mu
        self.lmbda = lmbda
        self.mutation = mutation
        self.constraint = constraint

    def _optimize(
            self,
            X: np.ndarray,
            y: np.ndarray,
            initial_rule: Rule,
            random_state: RandomState,
    ) -> Optional[Rule]:
        rule_template = initial_rule.clone().fit(X, y)

        problem = RuleOptimizationProblem(
            rule_template,
            X,
            y, 
            constraint=self.constraint,
        )

        mutation = PymooRuleMutation(
            rule_template,
            mutation_operator=self.mutation,
            constraint=self.constraint,
            X_train=X,
            y_train=y,
            random_state=random_state,
        )

        algorithm = NSGA2(
            pop_size=self.lmbda,
            mutation=mutation,
            #mutation=PM(eta=50, prob=0.8),
            #crossover=SBX(eta=, prob=0.9),
            eliminate_duplicates=False,
        )

        termination = get_termination(
            "n_gen",
            self.n_iter
        )

         # ─────────────── START PROFILING ───────────────
        profiler = cProfile.Profile()
        profiler.enable()

        result = minimize(
            problem,
            algorithm,
            termination,
            seed=int(random_state.integers(0, 2**32 - 1)),
            verbose=False,
        )

        profiler.disable()
        # Dump the top 20 functions by cumulative time
        stats = pstats.Stats(profiler).sort_stats("cumtime")
        stats.print_stats(20)
        # ─────────────── END PROFILING ─────────────────

        population = result.algorithm.pop

        F = population.get("F")
        Xf = population.get("X")
        rank = population.get("rank")

    
        pf_idx = np.where(rank == 0)[0]
        if pf_idx.size == 0:
            return None

        best_idx = pf_idx[np.argmin(F[pf_idx, 0])]

        best_vec  = Xf[best_idx]
        best_rule = rule_template.clone().set_param_vector(best_vec)
        best_rule = best_rule.fit(X, y)

        return best_rule if best_rule.is_fitted_ and best_rule.experience_ > 0 else None
